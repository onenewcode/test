{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出模型整体结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_path = r\"E:\\gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model.eval()\n",
    "# model = model.half()  \n",
    "wte=1\n",
    "wpe=1\n",
    "# print(model)\n",
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"Layer: {name} | Shape Type: {param.shape}\")\n",
    "        # if name == \"transformer.ln_f.weight\":\n",
    "        #     print(f\"Layer: {name} | Shape Type: {param.shape}| Data Type: {param}\")\n",
    "        # if name == \"transformer.ln_f.bias\":\n",
    "        #     print(f\"Layer: {name} | Shape Type: {param.shape} | Data Type: {param}\")\n",
    "#             df = pd.DataFrame(param.numpy())\n",
    "#             # df.to_csv('output.csv', index=False)  \n",
    "#         # print(f\"Layer: {name} | Shape: {param.shape}\")\n",
    "#     # for name, module in model.named_modules():\n",
    "#     #      print(f\"Name: {name}, Module: {module}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM,LlamaModel\n",
    "model_path = r\"D:\\ProjectRust\\learning-lm-rs\\models\\story\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model.eval()\n",
    "# model = model.half()  \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化模型的计算流图\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel \n",
    "import torch\n",
    "# 定义一个钩子函数，用于打印每一层的输入和输出\n",
    "model_path = r\"E:\\gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).eval()\n",
    "# 遍历所有子模块并注册钩子\n",
    "\n",
    "# 手动构建张量\n",
    "inputs = torch.tensor([])\n",
    "print(tokenizer.decode([717,198,717]))\n",
    "\n",
    "# model.eval()\n",
    "# model.generate(inputs, max_length=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# 加载模型和分词器\n",
    "model_path = r\"E:\\gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).eval()\n",
    "\n",
    "def find_layernorm_eps(module, prefix=\"\"):\n",
    "    \"\"\"递归地查找所有的 LayerNorm 层并打印其 epsilon 值\"\"\"\n",
    "    for name, child in module.named_children():\n",
    "        full_name = f\"{prefix}.{name}\" if prefix else name\n",
    "        if isinstance(child, torch.nn.LayerNorm):\n",
    "            print(f\"LayerNorm found at {full_name} with epsilon: {child.eps}\")\n",
    "        find_layernorm_eps(child, full_name)\n",
    "\n",
    "# 打印所有 LayerNorm 层的 epsilon 值\n",
    "find_layernorm_eps(model)\n",
    "\n",
    "# 如果你想要获取特定 LayerNorm 层的 epsilon 值，可以直接访问该层\n",
    "# 例如，假设我们知道某个 LayerNorm 层的位置如下：\n",
    "# layer_norm = model.transformer.h[0].ln_1\n",
    "# print(layer_norm.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造回调函数，用于输出每一层的计算结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel \n",
    "import torch\n",
    "def print_tensor_elements(tensor, label=\"Tensor\", num_elements=5):\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        elements = tensor.flatten()\n",
    "        print(f\"{label}: shape={tensor.shape}\")\n",
    "        print(\"First 5 elements:\", elements[:num_elements].tolist())\n",
    "        print(\"Last 5 elements:\", elements[-num_elements:].tolist() if len(elements) >= num_elements else elements.tolist())\n",
    "    else:\n",
    "        print(f\"{label}: Not a Tensor\")\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    print(f\"Layer: {module.__class__.__name__}\")\n",
    "\n",
    "    # 处理输入张量\n",
    "    if input and isinstance(input[0], torch.Tensor):\n",
    "        print_tensor_elements(input[0], label=\"Input\")\n",
    "    else:\n",
    "        print(\"error\")\n",
    "    # 处理输出张量\n",
    "    if input and isinstance(output[0], torch.Tensor):\n",
    "        print_tensor_elements(output[0], label=\"Input\")\n",
    "    else:\n",
    "        print(\"error\")\n",
    "\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "model_path = r\"E:\\gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).eval()\n",
    "# 遍历所有子模块并注册钩子\n",
    "hooks = []\n",
    "for name, module in model.named_modules():\n",
    "    if not isinstance(module, (torch.nn.ModuleList, torch.nn.Sequential)):\n",
    "        hooks.append(module.register_forward_hook(hook_fn))\n",
    "        # if name == 'transformer.ln_f':\n",
    "        #     hooks.append(module.register_forward_hook(hook_fn))\n",
    "\n",
    "# 手动构建张量并进行推理\n",
    "inputs = \"Once upon a time,\"\n",
    "generated_tokens = torch.tensor([[0]])\n",
    "# 将文本转换为模型输入\n",
    "input_ids = tokenizer(inputs, return_tensors='pt').input_ids\n",
    "\n",
    "# 使用模型进行推理\n",
    "with torch.no_grad():  # 确保推理过程中不计算梯度以节省内存\n",
    "    outputs = model.generate(generated_tokens, max_length=10, do_sample=True)\n",
    "print(outputs)\n",
    "for i in range(outputs.shape[0]):  # 遍历所有生成的序列\n",
    "    print(tokenizer.decode(outputs[i], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "model_path = r\"E:\\gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).eval()\n",
    "\n",
    "# 已经生成的token列表\n",
    "generated_tokens = [4874, 2402, 257, 640, 11]\n",
    "\n",
    "# 将token列表转换为tensor\n",
    "input_ids = torch.tensor([generated_tokens])\n",
    "\n",
    "# 设置生成参数\n",
    "generation_kwargs = {\n",
    "    'max_length': len(generated_tokens) + 30,  # 基于已有的tokens再生成30个\n",
    "    'min_length': len(generated_tokens) + 30,  # 确保至少生成30个新词\n",
    "    'num_beams': 5,                             # 使用波束搜索，值越大生成质量越高但速度越慢\n",
    "    'no_repeat_ngram_size': 2,                  # 避免重复的n-gram\n",
    "    'early_stopping': True                      # 如果达到良好的结果就提前停止生成\n",
    "}\n",
    "\n",
    "# 使用模型进行推理并生成指定数量的词\n",
    "with torch.no_grad():  # 推理时不需要计算梯度\n",
    "    output = model.generate(input_ids, **generation_kwargs)\n",
    "\n",
    "# 解码模型生成的输出\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"模型生成的文本:\", decoded_output)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "model_path = r\"E:\\gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).eval()\n",
    "\n",
    "for name, layer in model.named_modules():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda test\n",
    "import torch\n",
    "\n",
    "# 检查是否有可用的CUDA设备\n",
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# 打印PyTorch版本\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# 获取当前默认的CUDA设备\n",
    "if torch.cuda.is_available():\n",
    "    current_device = torch.cuda.current_device()\n",
    "    print(f\"Current CUDA device: {current_device}\")\n",
    "    print(f\"Name of the current CUDA device: {torch.cuda.get_device_name(current_device)}\")\n",
    "else:\n",
    "    print(\"No CUDA devices available.\")\n",
    "\n",
    "# 列出所有可用的CUDA设备数量\n",
    "print(f\"Number of available CUDA devices: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel \n",
    "import torch\n",
    "# 加载预训练模型和分词器\n",
    "model_path = r\"E:\\gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).eval()\n",
    "\n",
    "def generate_text_word_by_word(model, tokenizer, start_text, max_length=50):\n",
    "    # 将起始文本编码为模型输入\n",
    "    input_ids = tokenizer.encode(start_text, return_tensors='pt')\n",
    "\n",
    "    generated = []\n",
    "    for _ in range(max_length):\n",
    "        # 模型推理\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "            # 对最后一个token应用softmax获取概率分布\n",
    "            next_token_probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "            # 选择具有最高概率的下一个token\n",
    "            next_token_id = torch.argmax(next_token_probs, dim=-1)\n",
    "\n",
    "            # 如果next_token是EOS（结束符号），则停止生成\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            # 添加到生成的列表中，并更新input_ids以便下一轮迭代使用\n",
    "            generated.append(next_token_id.item())\n",
    "            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
    "\n",
    "    # 解码生成的token id为文本\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "# 示例文本\n",
    "start_text = \"Once upon a time,\"\n",
    "\n",
    "# 逐词生成文本\n",
    "generated_text = generate_text_word_by_word(model, tokenizer, start_text, max_length=90)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换半精度\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_path = r\"E:\\gpt2\"\n",
    "model_path1 = r\"E:\\gpt2t\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model = model.half()\n",
    "model.save_pretrained(model_path1)\n",
    "tokenizer.save_pretrained(model_path1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

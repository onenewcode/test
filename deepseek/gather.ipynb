{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_path = \"/home/ztf/cpm\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,trust_remote_code=True)\n",
    "model.eval()\n",
    "print(model)\n",
    "# with torch.no_grad():\n",
    "#     for name, param in model.named_parameters():\n",
    "#         print(f\"Layer: {name} | Shape Type: {param.shape}\")\n",
    "        # if name == \"transformer.ln_f.weight\":\n",
    "        #     print(f\"Layer: {name} | Shape Type: {param.shape}| Data Type: {param}\")\n",
    "        # if name == \"transformer.ln_f.bias\":\n",
    "        #     print(f\"Layer: {name} | Shape Type: {param.shape} | Data Type: {param}\")\n",
    "#             df = pd.DataFrame(param.numpy())\n",
    "#             # df.to_csv('output.csv', index=False)  \n",
    "#         # print(f\"Layer: {name} | Shape: {param.shape}\")\n",
    "#     # for name, module in model.named_modules():\n",
    "#     #      print(f\"Name: {name}, Module: {module}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "def print_tensor_elements(tensor, label=\"Tensor\", num_elements=5):\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        elements = tensor.flatten()\n",
    "        print(f\"{label}: shape={tensor.shape}\")\n",
    "        print(\"First 5 elements:\", elements[:num_elements].tolist())\n",
    "        print(\"Last 5 elements:\", elements[-num_elements:].tolist() if len(elements) >= num_elements else elements.tolist())\n",
    "    else:\n",
    "        print(f\"{label}: Not a Tensor\")\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    print(f\"Layer: {module.__class__.__name__}\")\n",
    "\n",
    "    # 处理输入张量\n",
    "    if input and isinstance(input[0], torch.Tensor):\n",
    "        print_tensor_elements(input[0], label=\"Input\")\n",
    "    else:\n",
    "        print(\"error\")\n",
    "    # 处理输出张量\n",
    "    if input and isinstance(output[0], torch.Tensor):\n",
    "        print_tensor_elements(output[0], label=\"Input\")\n",
    "    else:\n",
    "        print(\"error\")\n",
    "\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "model_path = \"/home/ztf/cpm\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model =  AutoModelForCausalLM.from_pretrained(model_path,trust_remote_code=True)\n",
    "model.eval()\n",
    "# 遍历所有子模块并注册钩子\n",
    "hooks = []\n",
    "for name, module in model.named_modules():\n",
    "    if not isinstance(module, (torch.nn.ModuleList, torch.nn.Sequential)):\n",
    "        hooks.append(module.register_forward_hook(hook_fn))\n",
    "        # if name == 'transformer.ln_f':\n",
    "        #     hooks.append(module.register_forward_hook(hook_fn))\n",
    "\n",
    "# 手动构建张量并进行推理\n",
    "inputs = \"Once upon a time,\"\n",
    "generated_tokens = torch.tensor([[59422]])\n",
    "# 将文本转换为模型输入\n",
    "input_ids = tokenizer(inputs, return_tensors='pt').input_ids\n",
    "\n",
    "# 使用模型进行推理\n",
    "with torch.no_grad():  # 确保推理过程中不计算梯度以节省内存\n",
    "    outputs = model.generate(generated_tokens, max_length=2, do_sample=True)\n",
    "print(outputs)\n",
    "for i in range(outputs.shape[0]):  # 遍历所有生成的序列\n",
    "    print(tokenizer.decode(outputs[i], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
